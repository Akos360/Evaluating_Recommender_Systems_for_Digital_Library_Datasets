\newcommand{\myFontSize}[0] {10pt}
\newcommand{\mySpacing}[0] {1.3}

\newcommand{\thesisTitle}[0] {Evaluating Recommender Systems for Digital Library Datasets}
%Odporúčacie systémy založené na AI
\documentclass[\myFontSize,oneside,english,hidelinks,a4paper]{article} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{url} 
\usepackage{doi} 
\usepackage{cite}
\usepackage{comment} 
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage[a4paper, top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{glossaries}
\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc}

\usepackage{lipsum}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{listings}
\usepackage{xcolor}  % for custom colors

\definecolor{codegray}{gray}{0.95}
\lstset{
  backgroundcolor=\color{codegray},
  basicstyle=\ttfamily\normalsize,
  frame=single,
  breaklines=true,
  showstringspaces=false,
  tabsize=2,
  language=Python
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setstretch{\mySpacing}
\setcounter{tocdepth}{3}

\makeglossaries

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document} 


\begin{center}
\thispagestyle{empty}
{\Large Slovak University of Technology in Bratislava}
\par\end{center}{\Large \par}

\begin{center}
{\Large Faculty of Informatics and Information Technologies} 
\par\end{center}{\Large \par}

\smallskip{}

\vfill{}

\begin{center}
\textbf{\Large Ákos Lévárdy}
\par\end{center}{\Large \par}

\medskip{}

\begin{center}
\textbf{\Large \thesisTitle}
\par\end{center}{\Large \par}

\medskip{}

\begin{center}
{\Large Bachelor thesis}
\par\end{center}{\Large \par}

\vfill{}

\Large{
Degree course: Informatics\\ 
Field of study: 9.2.1 Informatics\\ 
Place: FIIT STU, Bratislava\\ 
Supervisor: PaedDr. Pavol Baťalík\\
\today}

%\maketitle
%\thispagestyle{empty}
%\vspace{4\baselineskip} 		% vertical space 
%\hspace{-2cm} 					% horizontal position
%\parbox{0.8\textwidth}{ 
%\raggedright 					% Left-align the text
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\titleformat{\section}{\LARGE\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\Large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\Large\bfseries}{\thesubsubsection}{1em}{}

%
%\tiny
%\scriptsize
%\footnotesize
%\small
%\normalsize
%\large
%\Large
%\LARGE
%\huge
%\Huge
%

\newpage{}
\thispagestyle{empty}
\mbox{}

\newpage{} 
\thispagestyle{empty}
\section*{ANNOTATION}
\begin{minipage}[t]{1\columnwidth}%
Slovak University of Technology Bratislava 

Faculty of Informatics and Information Technologies

Degree Course: Informatics\\

Author: Ákos Lévárdy

Diploma Thesis: \thesisTitle

Supervisor: PaedDr. Pavol Baťalík

\today%
\end{minipage}
\bigskip{}

% 984 characters
This bachelor thesis focuses on analysing and evaluating recommender systems best suitable for digital library datasets. With the rise of digital content, these systems help users navigate large information spaces by generating personalized recommendations. In the introduction we showed the importance and role of Recommender Systems (RS) in filtering and anticipating user preferences across various domains. We then described the different recommendation techniques in detail, showing how they work and what their difficulties are. The project emphasizes Content-Based Filtering techniques, comparing algorithms like BERT, FastText, LSA, and TF-IDF. Offline experiments tested the system’s ability to provide Top-N book recommendations from the input data and the algorithms were compared based on evaluation metrics such as Similarity, Coverage, Diversity, and Confidence. This work aims to evaluate different recommendation algorithms designed for digital library datasets.

\newpage{}
\thispagestyle{empty}
\mbox{}

\newpage{}
\thispagestyle{empty}
\section*{ANOTÁCIA}
\begin{minipage}[t]{1\columnwidth}%
Slovenská technická univerzita v Bratislave

Fakulta informatiky a informačných technológií

Študijný program: Informatika\\

Autor: Ákos Lévárdy

Diplomová práca: \thesisTitle

Vedúci diplomového projektu: PaedDr. Pavol Baťalík

\today%
\end{minipage}
\bigskip{}

% 1037 characters
Táto bakalárska práca sa zameriava na analýzu a hodnotenie odporúčacích systémov, ktoré sú najvhodnejšie pre datasety digitálnych knižníc. S rastom digitálneho obsahu tieto systémy pomáhajú používateľom orientovať sa v rozsiahlych informačných priestoroch generovaním personalizovaných odporúčaní. V úvode sme ukázali význam a úlohu odporúčacích systémov pri filtrovaní a predvídaní preferencií používateľov v rôznych oblastiach. Následne sme detailne opísali rôzne techniky odporúčania, vrátane ich fungovania a výziev. Projekt kladie dôraz na techniky založené na obsahu (Content-Based Filtering), pričom porovnáva algoritmy ako BERT, FastText, LSA a TF-IDF. Offline experimenty testovali schopnosť systému poskytovať Top-N odporúčania kníh z vložených dát a algoritmy boli porovnávané na základe hodnotiacich metrík, ako sú podobnosť (Similarity), pokrytie (Coverage), rozmanitosť (Diversity) a dôvera (Confidence). Cieľom tejto práce je zhodnotiť rôzne odporúčacie algoritmy navrhnuté pre datasety digitálnych knižníc.

%\newpage{} 		% Čestné vyhlásenie
%\setcounter{page}{4}
%\vspace*{\fill}
%\noindent \Large \textbf{DECLARATION OF OATH}\\
%\noindent I hereby declare upon my honour that I wrote this thesis single-handed with usage %of quoted literature and based on my knowledge and professional supervision of my supervisor.
%\vspace*{\fill} 
%\vspace{-8cm} 


\pagenumbering{roman}
\newpage 			% Poďakovanie
\setcounter{page}{4}
\vspace*{\fill} 
\noindent \Large \textbf{ACKNOWLEDGMENT}\\
\noindent First and foremost, I would like to thank my supervisor for their invaluable guidance and support throughout the duration of this project.
\vspace*{\fill} 
\vspace{-8cm} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage{} 
\setcounter{page}{5}
\renewcommand{\contentsname}{Table of Contents}
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newacronym{ai}{AI}{Artificial Intelligence}
%\newacronym{ml}{ML}{Machine Learning}

\newpage{}
\listoffigures
\listoftables
\printglossary[type=\acronymtype, title=List of Abbreviations]







%\section*{List of Abbreviations}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \hspace{1.3em}

\clearpage{} 
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}
%artificial \gls{ai}\\
%machine \gls{ml}\\

As Internet and Web technologies continue to evolve rapidly, the amount of information available online has expanded excessively across sections such as e-commerce, e-government or e-learning. To help users navigate this vast sea of content, Recommender Systems (RS) have become fundamental. These systems are not designed just for saving time for users, but they are enhancing the users experience when using the said system, by anticipating their needs and relevant items or topics to discover. They are very effective tools for filtering out the most appropriate information any user would like to find. The primary focus of these recommendations is to predict if a specific user will be interested in the distinct items.\\
%
“Item” is the general term used to denote what the system recommends to users. A RS normally focuses on a specific type of item (e.g., movies, books or news) and accordingly its design, its graphical user interface, and the core recommendation technique used to generate the recommendations are all customized to provide useful and effective suggestions for that specific type of item. \cite{pub.1036183961}\\
%
The basic principle of recommendations is that significant dependencies exist between user- and item-centric activity. 
For example, a user who is interested in a historical documentary is more likely to be interested in another historical documentary or an educational program, rather than in an action movie. \cite{pub.1022525812}\\\\
%
The main target of this project is to create a recommendation system that uses different algorithms for book recommendations and evaluate these algorithms based on specified metrics.\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage{}
\section{Understanding Recommendation Systems}
Making decisions is not always easy. People are frequently presented with an overwhelming number of options when picking a product, a movie, or a destination to travel to, and each option comes with different levels of information and trustworthiness. \\
While there are many situations in which users know exactly what they are looking for and would like immediate answers, in other cases they are willing to explore and extend their knowledge \cite{Blanco201333}.\\\\
The main purpose of \textbf{Recommendation Systems} is to predict useful items, select some of them and after comparing them, the system recommends the most accurate ones.\\ 
These Personalized recommendation systems are emerging as appropriate tools to aid and speed up the process of information seeking, considering the dramatic increase in big data \cite{Haruna2017}. They need to handle a large amount of textual data in order to accurately understand users’ reading preferences and generate corresponding recommendations \cite{Yan2024}. \\
%
Because of this number of detail from all of the items, recommendation systems are becoming increasingly important. They help reduce options and offer better suggestions for the user so that they will have a personalized list to select their favourite. Fast and efficient access to information is essential in any field of study. \\\\
%
Similarly, \textbf{Search Engines} are essential for navigating the vast amount of information available online. They make it possible for people to quickly look up solutions, learn new things, and browse the wide variety of resources on the internet. Search engine optimization is now necessary to guarantee that search engines deliver relevant results, quick search times, and a top-notch user experience given the explosive growth of online information.\\
A search engine is essentially a software that finds the information the user needs using keywords or phrases. It delivers results rapidly, even with millions of websites available online.
The importance of speed in online searches is highlighted by how even minor delays in retrieval can negatively affect users' perception of result quality \cite{pub.1171882357}.\\
%
While both Recommendation Systems (Information Filtering techniques) and Search Engines (Information Retrieval techniques) aim to help users navigate all this information, they do it differently. Personalized recommendation systems make suggestions based on past user behavior and preferences, whereas search engines use keyword-based searches to retrieve content from a selection of sources.\\\\
%
Information systems often deal with changing data over time. The term called Concept drift describes when sometimes the patterns or behaviors in the data change unexpectedly which affects how the system makes predictions \cite{Sun2024}.\\
The task to provide users with currently available options for products that fit their requirements and interests is very important in todays consumer society. These products are mostly supplied by inputs \cite{Philip2014} , sometimes even matching the users distinct tastes.\\\\
%
When someone is trying to find a movie to watch, it would be hard for them to start searching without any starting options. After all a blank page and no suggestions to choose from might even make the user decide not to pick anything. \\\\
%
\newpage{}
Recommending items can be done in a variety of ways. Several types of recommendation systems exist, and their methods of operation differ. Here are the different recommendation systems\\

%These recommendation types can be divided into 3 main categories, which are Content-Based Filtering approaches (CB), Collaborative Filtering approaches (CF) and Hybrid approaches which are the combinations of the two. \\
%Other categories also include Knowledge-Based, Context-Aware, Popularity-Based, Demographic, Utility-Based and Deep Learning-Based Recommendation.\\\\
%

%{
%\begin{table}[h!]
%\centering
%\renewcommand{\arraystretch}{1.3} 
%{\Large
%\begin{tabular}{m{0.03\textwidth}|m{0.5\textwidth}}
%\hline
%\textbf{\# } & \textbf{Category} \\ \hline
%1. & Content-Based Filtering (CB) \\ \hline
%2. & Collaborative Filtering (CF) \\ \hline
%3. & Hybrid Approaches \\ \hline
%4. & Knowledge-Based \\ \hline
%5. & Context-Aware \\ \hline
%6. & Popularity-Based \\ \hline
%7. & Demographic \\ \hline
%8. & Utility-Based \\ \hline
%9. & Deep Learning-Based \\ \hline
%\end{tabular}
%}
%\caption{Types of Recommendation Systems}
%\label{table:recommendation-systems}
%\end{table}
%}


%
\textbf{Basic ideas of the recommendation techniques:}
\begin{itemize}[label=--]
\item \textbf{Content-Based Filtering} works in a way that it creates user profiles and suggests the individual items or products based on the users past choices with similar items. The items have various features and characteristics which connect them \cite{pub.1034486657}.
\item \textbf{Collaborative Filtering} relies more on preferences of other users and their behaviour. The point is that users who had similar interests before will have them again in the future for new items \cite{NILASHI2018507}.
\item \textbf{Knowledge-Graphs} use a network of data where items are linked through their features. Showing how items relate to one another and connecting them with more information and detail \cite{Imene2022488}.
\item \textbf{Context-Aware} recommendation systems are adding contextual factors to the rating process, where the recommended item is based on the users explicit ratings, the items implicitly inferred ratings and also the contextual variables  \cite{Haruna2017}. The variables for example when recommending a movie can be the location from where the user watches the movie, the time and the companion who the user watches the movie with.
\item \textbf{Popularity-Based} recommendations offer products that are popular or well-liked by a lot of users. They assume that these popular items are likely to be of interest to the majority of users, not considering their personal preferences.
\item \textbf{Demographic} recommendation systems are recommending items based on a demographic profile of the user. They categorize the users from their personal attributes and try to make user stereotypes \cite{Burke2002331}.
\item \textbf{Utility-Based} systems generate the recommendations by computing the utility of each item for the user. The utility of an item refers to how valuable it is to a user and is calculated using a utility function which combines different factors of the user's preferences \cite{Burke2002331}.
\item \textbf{Deep Learning-Based} are trying to find complex patterns in the users behaviour and the items features using deep learning algorithms and neural networks. These models can locate hidden links and can offer highly customized recommendations.
\item \textbf{Hybrid methods} try to combine the useful characteristics of both collaborative filtering and content-based filtering methods. They take into account both the users past preferences and the preferences of other people who might share the users taste \cite{melville:aaai02}. \\
\end{itemize}
%
%
%
%
%
\newpage{}
\subsection{Role of Recommendation Systems}
The Recommendation System can have a range of roles to play. First it is important to distinguish on whose behalf the role is played, which can be either the service providers or the users side. For example, a recommendation system for music, implemented by a streaming service like Spotify wants to increase user engagement by recommending new playlists and songs, which leads to more subscriptions or advertisement revenue. While on the other hand the user wants to listen to personalized playlists and discover songs they might like.\\\\
There are more ways why a service provider would want to utilize such technology:
\begin{itemize}
\item Sell more items - to be able to sell additional items beyond those which are normally sold without recommendations. To increase the number of users that accept a recommendation and consume an item.
\item Sell more diverse items - not just to sell the most popular items, but also recommend items that might be hard to find. The popular items will probably be sold either way, on the other hand the service provider might want to sell every item.
\item Increase user satisfaction - improve the experience for the user with effective recommendations and combine it with a usable interface, so the user will be more satisfied with the system.
\item Increase user fidelity - make more personalized recommendations based on the users previous visits and interactions, by treating the user as a valuable customer.
\item Better understand what the user wants - to describe the user's preferences which are explicitly collected or predicted by the system. The service provider can even use this knowledge for other goals like improve inventory management or target specific promotions. \cite{pub.1036183961}\\
\end{itemize}
%
From the users point of view the recommendation system can help in implementing other core tasks which are normally associated with an RS. The popular tasks are the following \cite{Ricci20221}:\\

\begin{table}[h!]
\centering
{
\renewcommand{\arraystretch}{2}
{\large
\begin{tabular}{|m{0.03\textwidth}|m{0.3\textwidth}|m{0.6\textwidth}|}
\hline
\textbf{\#} & \textbf{Task} & \textbf{Description} \\ \hline
1. & Find some good items & Identify a selection of quality items. \\ \hline
2. & Find all good items & Locate all available items deemed good. \\ \hline
3. & Annotate items in context & Emphasize items based on the user's preferences and context. \\ \hline
4. & Recommend a sequence & Suggest an order for engaging with items. \\ \hline
5. & Recommend a bundle & Propose a set of complementary items together. \\ \hline
6. & Just browsing & Explore items without the intention of purchasing. \\ \hline
7. & Find credible recommender & Evaluate how effective the system is at making recommendations. \\ \hline
8. & Improve the active user's profile & Enhance the system's understanding of the user's preferences. \\ \hline
9. & Express self & Share opinions and provide ratings to assist the system. \\ \hline
10. & Help others & Evaluate items to guide others in finding what suits them. \\ \hline
11. & Influence others & Persuade other users to consider particular products. \\ \hline
\end{tabular}
}
}
\caption{Recommendation System tasks}
\label{table:rs-tasks}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\subsection{Recommendation Techniques}
As mentioned before recommendation techniques are generaly categorized into three approaches which are Collaborative Filtering, Content-Based Filtering and Hybrid Approaches. These methods differ in how they generate recommendations and offer unique advantages. The efficiency of a recommender system greatly depends on the type of algorithm used and the nature of the data source, which may be contextual, textual, visual etc. \cite{Roy2022}\\\\
In the following section the techniques Collaborative Filtering, Content-based Filtering, Knowledge Graphs and Hybrid Approaches are described in more detail.\\
%
\subsubsection{Collaborative Filtering}
One of the most popular methods used for personalized recommendations is collaborative filtering. This method filters information from users, which means it compares users behaviour, interactions with items and data, item correlation and ratings from users. \\
It can perform in domains where there is not much content associated with items, or where the content is difficult for a computer to analyze - ideas, opinions etc.\cite{melville:aaai02}\\
Collaborative filtering can be divided into 2 methods which are "Memory-based" and "Model-Based" collaborative filtering. The first one relies on historical preferences, whereas the second method is based on machine learning models to predict the best options.\\\\
%
\textbf{Memory-based CF}\\
Recommender systems based on memory automate the common principle that similar users prefer similar items, and similar items are preferred by similar users \cite{Ning201537}. \\
Memory-based collaborative filtering, which can also be called Neighborhood-based is further divided into 2 basic types, which are:
\begin{itemize}
\item User-Based Collaborative Filtering
	\begin{itemize}
	\item The main idea is that 2 completely distinct users who have an interest in a specific item and they rate this item similarly will probably be drawn to a new item the same way.
	\end{itemize}
\item Item-Based Collaborative Filtering
	\begin{itemize}
	\item Calculates similarity between items, rather than users. The user will probably like a new item which is similar to another item they were interested in before.\\
	\end{itemize}
\end{itemize}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{img/collaborative_example.png}
    \caption{Illustration of Memory-based CF recommendation}
    \label{fig:collaborative_example}
\end{figure}
%
%
When trying to implement this type of recommendation system it is important to consider the key components, which are: 
\begin{itemize}
\item Rating Normalization - adjusts individual user ratings to a standard scale by addressing personal rating habits. Using for example Mean-Centering or Z-Score Normalization. 
\item Similarity Weight Computation - helps to select reliable neighbors for prediction and deciding how much impact each neighbor's rating has. A lot of Similarity measures can be used, such as Correlation-Based Similarity, Mean Squared Difference or Spearman Rank Correlation. 
\item Neighborhood Selection - selects the most appropriate candidates for making predictions based on each unique scenario, eliminating the least likely ones to leave only the best options. \cite{Ning201537}
\end{itemize}
%
%
\textbf{Model-based CF}\\
Recommender systems based on models, also known as Learning-based methods, try to develop a parametric model of the relationships between items and users. These models can capture patterns in the data, which can not be seen in the previous recommendation type. \\\\
Model-based algorithms do not suffer from memory-based drawbacks and can create prediction over a shorter period of time compared to memory-based algorithms because these algorithms perform off-line computation for training. 
The well-known machine learning techniques for this approach are matrix factorization, clustering, probabilistic Latent Semantic Analysis (pLSA) and machine learning on the graph \cite{NILASHI2018507}. \\\\
%
%
%
\textbf{Matrix Factorization}\\
In its basic form, matrix factorization characterizes both items and users by vectors of factors inferred from item rating patterns. High correspondence between item and user factors leads to recommendations \cite{5197422}. \\
People prefer to rate just a small percentage of items, therefore the user-item rating matrix, that tracks the ratings people assign to various items, is frequently sparse.\\
In order to deal with this sparsity, matrix factorization (MF) algorithms split the matrix into two lower-rank matrices: one that shows the latent properties of the items and another that reflects the underlying user preferences. These latent representations can be used to predict future ratings or complete the matrix's missing ratings after factorization \cite{Tokala2023}.\\\\
%
%
%
It is important to mention that the effectiveness depends on the ratio of users and items. For example when trying to recommend songs, there are usually way more users than songs and generally, many users listened to the same songs or same genres. Which means like-minded users are found easily and the recommendations will be effective. On the other hand, in a different field for example, when recommending books or articles the systems deals with millions of articles but a lot less users. This leads to less ratings on papers or no ratings at all, so it is harder to find people with shared interests \cite{Beel2016305}.\\








\subsubsection{Content-Based Filtering}
Recommender Systems which are using content-based filtering, review a variety of items, documents and their details. Each product has their own description which is collected to make a model for each item. The model of an item is composed by a set of features representing its content. \\
The main benefit of content-based recommendation methods is that they use obvious item features, making it easy to quickly describe why a particular item is being recommended. \cite{pub.1034486657}\\
This also allows for the possibility of providing explanations that list content features that caused an item to be recommended, potentially giving readers confidence in the system’s recommendations and insight into their own preferences \cite{Mooney2000195}. \\
These profiles for items are different representations of information and users interest about the specific item. \\
The recommendation process basically consists in matching up the attributes of the user profile against the attributes of a content object. \cite{pub.1034486657}\\
%
%
Some additional side information about items can be also useful, where this side information predominantly contains additional knowledge about the recommendable items, e.g., in terms of their features, metadata, category assignments, relations to other items, user-provided tags and comments, or related textual content. \cite{Lops2019239}\\\\
The process for recommending items using content-based filtering has 3 different phases and this high level architecture is shown in Fig.~\ref{fig:high_lvl_content_based}.:
\begin{itemize}
\item Content Analyzer - Turns the unstructured information (text) into structured, organized information using pre-processing steps which are basic methods in Information Retrieval, such as feature extraction.
\item Profile Learner - Collects data of the users preference (feedback) that can be either positive information reffering to features which the active user likes or negative ones which the user does not like. After generalization it tries to construct user profiles for later use.
\item Filtering Component - Matches the items for the user, based on the similarities between item representations and user profiles, meaning it compares the features of new items with features in user preferences that are stored in the users profile. \cite{pub.1034486657}
\end{itemize}
%

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/content_based_figure_1.png}
    \caption{High level architecture of a content-based recommender \cite{Musto2022251}}
    \label{fig:high_lvl_content_based}
\end{figure}

The user modeling process has the goal to identify what are the users needs and this can be done 2 ways. Either the system calculates them from the interactions between the user and items through feedback or the user can specify these needs directly by giving keywords to the system, providing search queries \cite{Beel2016305}. \\\\
%
%
\textbf{Feedback}\\
When trying to acquire helpful information or criticism that is given by the user there are 2 separate ways. \\
The first one is called Explicit Feedback where it is necessary for the user to give item evaluation or actively rate products. Most popular options are gathering like/dislike ratings on items or the ratings can be on a scale either from 1 to 5 or 1 to 10. After the ratings the user can also give comments on separate items. \\\\
The other way is called Implicit Feedback where the information is collected passively from analyzing the users activities. Some alternatives can be clicks on products, time spent on sites or even transaction history \cite{DeGemmis2015119}.\\\\
%
%
%
\textbf{Advantages and Disadvantages of CB Filtering}
\begin{itemize}
\item User Idependence - meaning the ratings taken into consideration are only provided by the active user to build their own profile. Collaborative approaches will recommend items based on ratings from other users in the nearest neighborhood.
\item Transparency - explanations for the recommended items can be provided explicitly by listing the content features which were used to get that recommendation. On the other hand Collaborative systems are considered black boxes, where explanations are based on similar tastes of different users.
\item New Item - when a new item is added to the system, the Content-based method is capable of recommending it from the set features and its content. This is not possible for the Collaborative method which needs ratings for the new item to be able to recommend it.
\item Limited Content Analysis - because the system can only analyze a certain number of features and can miss important aspects such as aesthetics or other multimedia information. Also, systems based on string matching approach can suffer from problems such as synonymy, polysemy or multi-word expressions.
\item Over-Specialization - the user will mostly be recommended things similar to what they already liked, which drawback is called 'lack of serendipity'. For example, if the user only rated action movies, then the system would not recommend other genres, which limits the chance of recommending items with novelty or surpise. \cite{DeGemmis2015119}\\
\end{itemize}
%
%
%
%
\textbf{Semantic approaches in CB Recommendation}\\
In short Semantics refer to interpretation of meaning in language, words and symbols. Using semantic techniques the represantations of items and user profiles shift from keyword-based to concept-based ones.  With these represantations it is possible to give meaning to information expressed in natural language and to get a deeper understanding of the information presented by textual content.\\
Content-based recommendations can adopt two different approaches based on how the semantics are derived and applied:
\begin{itemize}
\item \textbf{Top-Down Semantic Approaches} use an external knowledge to improve the representation of the items and users. This external knowledge can be: ontological resources, encyclopedic knowledge (ESA, BabelNet) and the Linked Open Data cloud. For example the \textbf{Ontology} is a structured description that shows how different parts of a system depend on each other and how they are connected. It organizes key concepts in a specific domain into a hierarchy, which can explain their relationships and the characteristics of each concept. It can help to understand how specific examples of these concepts behave and how they are related \cite{pub.1090632691}.
%
\item \textbf{Bottom-Up Semantic Approaches} use implicit semantic representation of items and user profiles, where the meanings of terms are assumed by analyzing its usage. They rely on the distributional hypothesis: "words that occur in the same context tend to have similar meanings". Without a predefined structure, this approach analyzes the words co-occurrence with other words, larger texts or documents using Discriminative Models.\cite{DeGemmis2015119}\\
\end{itemize}
%
%
The semantic approaches can be further categorized by the source of knowledge used to extract meaning, which are \textbf{Endogenous Semantics} and \textbf{Exogenous Semantics}.\\
In the first case, the semantics is obtained by exploiting unstructured data, and is directly inferred from the available information. Different techniques for these Implicit Semantics Representations are for example the Term Frequency - Inverse Document Frequency (TF-IDF) weighting, or the Distributional Semantics Models (DSM) such as Explicit Semantics Analysis (ESA), Random Indexing or Word Embedding Techniques.
Word embedding technology can reflect the semantic information of words to a certain extent. The semantic distance between words can be calculated by word vectors. Commonly used word vectors are based on Word2vec and Fasttext models \cite{Huang2023}.\\\\
%
In the second, the semantics comes from the outside, since it is obtained by mining and exploiting data
which are previously encoded in structured and external knowledge sources. For these Explicit Semantics Representations there are also different techniques like Linking Item Features to Concepts using Word Sense Disambiguation (WSD) or using Entity Linking, or Linking Items to Knowledge Graphs using Ontologies or Linked Open Data (LOD). \cite{Musto2022251} \\
The LOD cloud is a huge decentralized knowledge base where researchers and organizations publish their data in Resource Description Framework (RDF) format and adopt shared vocabularies, in order to express an agreed semantics and interlink the data to each other \cite{Musto2017405}.\\
%
%
%
%\clearpage
\subsubsection{Knowledge Graphs}
Knowledge graph is a knowledge base that uses a graph-structured data model. It is a graphical databases which contains a large amount of relationship information between entities and can be used as a convenient way to enrich users and items information \cite{Imene2022488}.\\
%
The idea is that attributes of users and items are not isolated but linked up with each other, which forms a knowledge graph (KG). Incorporating a Knowledge Graph into recommendations can help the results in ways like:
\begin{itemize}
\item The rich semantic relatedness among items in a KG can help explore their latent connections and improve the precision of results
\item The various types of relations in a KG are helpful for extending a user’s interests reasonably and increasing the diversity of recommended items
\item KG connects a user’s historically-liked and recommended items, thereby bringing explainability to recommender systems. \cite{Wang20193307}
\end{itemize}
%
Basically a knowledge graph is a directed graph whose nodes are the entities and the edges are the relations between them. They are usually defined as triplets with a head entity, tail entity and a relationship connecting them. The graphs have detailed supporting information which is background knowledge of items and their relations amongst them. The facts of items are organized in those triplets like (Ed Sheeran, IsSingerOf, Shape of You), which can be seamlessly integrated with user-item interactions. This interaction data is usually presented as a
bipartite graph \cite{pub.1120733877}. \\
The crucial point to leverage knowledge graphs to perform item recommendations is to be able to effectively model user-item relatedness from this rich heterogeneous network \cite{Palumbo201732}. \\ 
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/knowledge_graph_example.png}
    \caption{Illustration of KG-aware recommendation}
    \label{fig:knowledge_graph_example}
\end{figure}
%
%
\newpage{}
Recommendation methods based on knowledge graphs can be practically categorized into two different types which are Path-based and Embedding-based methods. Where the \textbf{Embedding-based} method uses knowledge graph embedding (KGE) techniques trying to learn how to represent users and items. This method uses representation learning to find connections implicitly, rather than using user preferences. Models such as TransE, TransR and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. These models simply put both entities and relations within the same semantic space. \cite{pub.1148917041}\\
The \textbf{Path-based} method focuses on enhancing the connections called meta-paths that link the users and items, showing how similar they are \cite{Yang20229308}. \\
%



\subsubsection{Hybrid Approaches}
The Hybrid recommender systems try to combine two or more recommendation techniques to get to better performance and accuracy. The most common approach is to have a collaborative filtering technique combined with some other technique to try to avoid the ramp-up problem.
The ramp-up problem actually means two problems, which are "New User" and "New Item" problems (hard to categorize with few ratings).\\
Different types of hybrid recommender systems exist, which are the following \cite{Burke2002331}:
\begin{itemize}[label=--]
\item \textbf{Weighted hybrid} - initially gives equal weight to all available recommendation techniques in the system. Gradually adjusts the weighting based on wether the predicted user ratings are confirmed or discomfirmed. The recommended items score is computed from the results.
\item \textbf{Switching hybrid} - the system switches between recommendation techniques based on some criterion. The advantage is that the system can adapt to the strengths and weaknesses of the different recommendation methods it combines.
\item \textbf{Mixed hybrid} - recommendations from more than one technique are presented together at the same time. 
\item \textbf{Feature combination} - for example in a content / collaborative merger the system treats the collaborative information as an additional feature data and uses content-based techniques over this enhanced dataset.
\item \textbf{Cascade hybrid} - one recommender produces a coarse ranking of candidates and than the other refines the recommendations given by the first one. The second step only focuses on the items given by the first step and not all items in the dataset.
\item \textbf{Feature augmentation} - one technique produces a rating of an item and that information is then incorporated into processing the next recommendation technique. The features used by the second recommender include the output of the first one (like ratings).
\item \textbf{Meta-level hybrid} - combines two techniques by using the model generated by one as the input for the other, meaning the entire model becomes the input.\\
\end{itemize}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Difficulties related to Recommendation Systems}
All types of recommendation systems encounter significant challenges which they have to face and issues they have to solve. Here are some main challenges:
%
\begin{itemize}
\item Cold-start problem - arises when making recommendations to new users and/or items for which the available information is limited. As a result, the recommendations offered in such cases tend to be of poor quality and lack usefulness.\cite{Al-Hassan2024a}
\item Data sparsity - when recommender systems use large datasets, the user-item matrix used for filtering can be sparse, which leads to worse performance of recommendations.
\item Scalability - as the number of users and items increases, so does the complexity of the algorithms used for recommending items.
\item Diversity - helps to discover new products, but some algorithms may accidentally do the opposite, which can also lead to lower accuracy in the recommendation process. \cite{pub.1072601078}
\item Privacy - because the information collected by the system usually includes sensitive information that users wish to keep private, users may have a negative impression if the system knows too much about them.
\item Serendipity - sometimes can be useful, but if the result of the recommendation system only has
serendipitous items and does not have related items, user may think that the system is not reliable. \cite{Aymen2022896}\\
%\item Exploration vs. Exploitation
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation of Recommendation Systems}
When trying to choose which recommendation approach is the best, first it is important to know the use case for the specific system. \\
The process of finding the most appropriate algorithm for the specific goal typically is based on experiments, comparing the performance of a number of candidate recommenders. Comparing the performance of an algorithm is mostly performed by using some evaluation metric, which usually uses numeric scores, that provides ranking of the compared algorithms. \\
For measuring the accuracy of predictions of the algorithm three classes of measurements are defined, which are \cite{Gunawardana2022547}:
\begin{itemize}
\item \textbf{Measuring Ratings Prediction Accuracy} wishes to measure the accuracy of the system's predicted ratings. The following metrics can be used in such situation: Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Normalized RMSE, Normalized MAE, Average RMSE, Average MAE.
\item \textbf{Measuring Usage Prediction} tries to recommend to users items that they may use, with the following metrics: Precision, Recall (True Positive Rate), False Positive Rate (1 - Specificity), F-measure, Area Under the ROC Curve (AUC).
\item \textbf{Ranking Measures} is not predicting an explicit rating, but rather is ordering items according to the user's preferences. This can be done Using a Reference Ranking like Normalized Distance based Performance Measure (NDPM), Average Precision (AP) correlation, Spearman's rank correlation coefficient, Kendall's rank correlation coefficient or using Utility-Based Ranking such as Normalized Discounted Cumulative Gain (NDCG), Discounted Cumulative Gain (DCG) or Average Reciprocal Hit Rank (ARHR). Other than that there is also Online Evaluation of Ranking.
\end{itemize}
%
%
%
However, not all users are trying to use the recommendation engine just for the most accurate predictions, but they might be more interested in other properties of the recommender system like \cite{Gunawardana2022547}:
\begin{multicols}{3}
\begin{itemize}
\item Coverage
\item Confidence
\item Trust
\item Novelty
\item Serendipity
\item Diversity
\item Utility
\item Risk
\item Robustness
\item Privacy
\item Adaptability
\item Scalability
\end{itemize}
\end{multicols}
%
%
%
In the domain of scientific publications, where users are relatively few with respect to the available documents, information needs and interests easily change in an unpredictable way over time due to evolving professional needs, there is no advertising pushing new items, and the long tail of infrequently read articles may contain the so-called sleeping beauties, that are documents containing extremely relevant results, but that remains unknown to most researchers for a very long time. \\
The Content-based approach does not require particular assumptions over the size and the activity of the user base. It does not penalize items that have less ratings or are less frequently consumed by many users as long as enough metadata are available, which even allows detailed explanations. These advantages over Collaborative Filtering techniques make this approach particularly attractive to the purpose of providing recommendation in the domain of scientific publications \cite{De_Nart201484}. \\
%
A study shows that more than half of the recommendation approaches applied Content-based filtering, when making recommendations for research papers and articles in libraries \cite{Beel2016305}. \\
%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{itemize}
%\item 
%\item 
%\end{itemize}


\clearpage
%
%
%\subsection{Hypotheses}
%A hypothesis is a specific description or prediction to define the evaluation's goal. It serves as the foundation for experiments showing the direction for testing and analyzing the results. The more precise the hypothesis, the clearer the setup of the evaluation.\\\\
%\textbf{Hypothesis 1:} Algorithms with contextual embeddings (BERT, FastText) will achieve higher \textbf{Usage Prediction} scores than simpler frequency-based methods (TF-IDF, BM25).\\
%%
%\textbf{Hypothesis 2:} Probabilistic models (LDA) will have higher \textbf{Coverage} compared to neural embedding techniques (Word2Vec, BERT), as they can better generalize relationships across sparse datasets.\\
%%
%\textbf{Hypothesis 3:} Transformer-based algorithms (BERT) will outperform other methods on \textbf{Ranking Measures} like NDCG because they leverage bidirectional encoding to better understand query-item relevance.\\
%%
%\textbf{Hypothesis 4:} Word2Vec and FastText will generate more \textbf{Diverse} recommendations than probabilistic or frequency-based methods, as their embeddings capture subtler relationships between less frequently co-occurring terms.\\
%%
%\textbf{Hypothesis 5:} GloVe and LSA will recommend items with higher \textbf{Novelty} scores compared to algorithms like BM25 or TF-IDF, as they infer latent relationships beyond direct term matches.\\
%%
%\textbf{Hypothesis 6:} BERT and Word2Vec will provide the highest \textbf{Confidence} in recommendations, as their embeddings are more precise and context-aware, resulting in smaller prediction uncertainty.\\




\section{Implementation Proposal}
% 5-10 Pages \\\\
Drawing from the theoretical foundation in the second section, this section proposes a practical framework for evaluating different Content-Based Filtering algorithms tested on recommending books from digital library datasets based on textual metadata and full-text features. We also describe the dataset we will be working with. The proposal includes a detailed explanation of our experiment, along with the rationale behind our choice of specific text represenation methods.\\ 
%

%
%

\subsection{Dataset Analysis}
The main objective of this project is to compare different recommendation algorithms for digital library datasets. To support this, a collection of academic books extracted from the digital library will be used. These books were converted from PDF format into structured JSON files, with the text segmented into individual sentences, paragraphs and pages. The experiments primarily focus on paragraph-level recommendations, as suggesting relevant content based on a single paragraph can be a valuable feature for navigating academic materials in a digital library.
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/json_example2.png}
    \caption{Example of extracted book paragraphs}
    \label{fig:extracted_paragraph_example}
\end{figure}\\
%
In addition to this dataset, a second dataset will be used to test the algorithms on simpler and more general types of text. This dataset, scraped from Goodreads, contains book metadata and short descriptions for a wide range of genres including fantasy, comedy, romance, and others. Unlike the academic content of the first dataset, this one includes more casual, narrative-driven books, which helps test the algorithms’ performance on short-form and less formal text.
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/rating_data_example2.png}
    \caption{Example of book descriptions dataset \cite{goodreads_kumar_2022}}
    \label{fig:book_description_dataset_example}
\end{figure}\\
%
By using 2 different datasets in experiments it ensures that the evaluation covers specialized and general-purpose recommendation use cases. Basic preprocessing such as tokenization, removal of special characters, and optional stopword filtering was applied prior to embedding.\\
%
%
%
\subsection{Text Representation Methods}
In order to build a robust and interpretable content-based recommendation system, I selected a diverse set of six text representation models: 
\begin{itemize}
\item TF-IDF
\item Bag of Words (BoW)
\item Latent Semantic Analysis (LSA)
\item GloVe
\item FastText
\item BERT (via SentenceTransformers)
\end{itemize}
%
These models were chosen to represent a broad spectrum of natural language processing techniques, ranging from classical statistical approaches to modern deep learning-based embeddings.\\
%
There were 2 goals behind this selection. First, to evaluate how different levels of semantic understanding affect recommendation quality and second, to establish clear baselines against which newer models could be compared to later. Each model has its own strengths that help us understand and use text similarity in recommendation tasks more effectively.\\
%
%
Below is an overview of the different methods that were used for text representation, highlighting their methodology and key characteristics:
\begin{enumerate}
\item \textbf{TF-IDF (Term Frequency - Inverse Document Frequency)}\\
TF-IDF was selected as a well-established and interpretable baseline for measuring textual similarity. It captures term importance based on document frequency and enables fast similarity computation using sparse vectors.
TF-IDF creates two scores that are interrelated, trying to figure out the relevancy of a given term (word) to a document given a larger body of documents. TF means how often a given word occurs in the given document, because words that occur frequently are probably more important. DF means how often the given word occurs in an entire set of documents, but this does not have to mean the word is important, it just shows common words that appear everywhere. So using Inverse DF shows how often the word appears in a document, over how often it appears everywhere \cite{pub.1022525812}.


\item \textbf{BoW (Bag of Words)}\\
BoW, while simple, provides a valuable lower bound for comparison. It does not consider word order and semantics, making it an a good contrast point to evaluate the improvements introduced by more complex models.
Bow creates a set of vectors containing the count of word occurrences in the document. Unlike TF-IDF, BoW just counts the occurrences of unique words and puts them in its vocabulary so each word becomes a feature or dimension. Each document is represented as a vector based on the frequency of words from the vocabulary. The term-document matrix represents the documents as rows and the unique words as columns with cells showing frequency.
 

\item \textbf{LSA (Latent Semantic Analysis)}\\
LSA extends TF-IDF by applying dimensionality reduction through Singular Value Decomposition (SVD) to uncover hidden patterns and relationships between terms and documents. This technique helps capture the underlying semantic structure of the text. By decomposing the term-document matrix into three smaller matrices representing topics, terms, and documents LSA identifies latent topics and represents documents in a lower-dimensional semantic space. \cite{Bergamaschi2015247}.


\item \textbf{GloVe (Global Vectors for Word Representation)}\\
GloVe was included as a pretrained static word embedding model that represents words as fixed-size vectors based on how often they appear together in a large text corpus. It builds a co-occurrence matrix where rows represent the words, columns represent the context words and each cell contains the frequency with which the word and context word co-occur within a specified window to learn the relationships between words, allowing it to capture their meanings and similarities. GloVe helps to test how well combining individual word vectors can represent the meaning of a full document.


\item \textbf{FastText}\\
FastText enhances traditional word embeddings by incorporating subword information, allowing the model to generate embeddings even for rare or morphologically complex words. FastText breaks words into character n-grams and learns embeddings for these subwords. It can handle words that are out of the vocabulary, because it models character n-grams and not words. For output it produces dense, fixed-length word vectors that have the additional subword information \cite{Yan2024}.


\item \textbf{BERT (Bidirectional Encoder Representations from Transformers - SentenceTransformer)}\\
BERT, implemented through the SentenceTransformers framework, represents the state-of-the-art in contextual language modeling. Unlike the other models, BERT produces embeddings that consider the full sentence context, allowing it to distinguish between different meanings of the same word depending on usage. It uses attention mechanisms to model relationships between all words in a sentence allowing bidirectional encoding.


%\item \textbf{LDA (Latent Dirichlet Allocation)}\\
%Is a generative model for document collections, unlike LSA, LDA is a probabilistic Topic Model, which decomposes a conditional term by the document porbability distribution into two different distributions, the term by topic distribution and the topic by document distribution. After running LDA, each document is represented as a topic distribution, i.e., a vector of probabilities indicating the degree to which each topic is present in the document. 

%\item \textbf{Word2Vec}\\
%Is a neural network-based algorithm that generates dense vector representations for words (embeddings). The neural network uses one hidden layer to create the embeddings, it comes in two main architectures. Using Continuous Bag of Words (CBoW) which predicts a target word based on the words around it, or using Skip-Gram which predicts the context words given a target word. The output is a vector for each word in the vocabulary.
%
%\item \textbf{Doc2Vec}\\
%Is an extension of Word2Vec also being a neural network-based algorithm, but is designed to generate dense vector representations for entire documents or sentences, not just words. It enhances the Word2Vec by adding a document vector, which represents the unique context of an entire document. Here are also two main architectures, the first is the Distributed Memory Model of Paragraph Vectors (PV-DM) which predicts a word within a context using the surrounding words and a document vector, and the second is the Distributed Bag of Words (PV-DBOW) which instead of predicting a word using context, it predicts context words using the document vector alone. For output each document is represented by a fixed-length dense vector, regardless of its size or content.

%\item \textbf{BM25 (Best Match 25)}\\
%Is a ranking function that builds on the TF-IDF model. It ranks a set of documents based on the query terms appearing in each document, not considering their proximity within the document.
\end{enumerate}


By including models from these various categories (statistical, latent, word-level embeddings, and contextual transformer-based representations) the experimental design ensures a comprehensive comparison across different levels of linguistic representation.


%
%
%\begin{table}[h!]
%\centering
%\renewcommand{\arraystretch}{2}
%{\large
%\begin{tabular}{|m{0.03\textwidth}|m{0.5\textwidth}|}
%\hline
%\textbf{\#} & \textbf{Feature Extraction Algorithms} \\ \hline
%1            & TF-IDF  \\ \hline
%2            & BoW  \\ \hline
%3            & GloVe \\ \hline
%4            & LSA \\ \hline
%5            & LDA  \\ \hline
%6            &  Word2Vec  \\ \hline
%7            & Doc2Vec   \\ \hline
%8            &  BERT  \\ \hline
%9            & FastText \\ \hline
%10           & BM25  \\ \hline
%\end{tabular}
%}
%\caption{Feature Extraction Methods}
%\label{table:feature_extraction}
%\end{table}
%
%
%
% \vspace*{2\baselineskip}
%\pagebreak{}
%\subsection{Descriptions of Similarity and Distance Measures:}
%Similarity and distance measures are critical for comparing items and identifying the most relevant recommendations. These measures quantify how closely two data points—such as the input items and the recommended items features—relate to each other in a multi-dimensional space. Different measures are suited for varying data types and use cases, impacting the effectiveness of the recommendation algorithm. Below are descriptions of the similarity and distance measures that will be used, each with its unique approach to comparing items or features:
%%
%%
%\begin{table}[H]
%\centering
%\renewcommand{\arraystretch}{1.5}
%{\large
%\begin{tabular}{|c|p{0.25\textwidth}|p{0.60\textwidth}|}
%\hline
%\textbf{\#} & \textbf{Measure} & \textbf{Description} \\ \hline
%1 & Cosine Similarity & Measures the cosine of the angle between two vectors in a multi-dimensional space. Ranges from -1 to 1 (-1 means completely opposite, 1 means completely similar). \\ \hline
%2 & Euclidean Distance & Calculated as the square root of the sum of squared differences between corresponding dimensions. It shows the straight-line distance between two points. \\ \hline
%3 & Jaccard Similarity & Measures the similarity between two sets by dividing the size of their intersection by the size of their union. \\ \hline
%4 & Manhattan Distance & Calculates the sum of the absolute differences between the coordinates of two points. \\ \hline
%5 & Pearson Correlation & Measures the linear correlation between two variables. Ranges from -1 to 1 (-1 means negative correlation, 1 means positive correlation, 0 means no linear relationship). \\ \hline
%6 & Bray-Curtis Distance & Measures the dissimilarity between two vectors as the sum of absolute differences divided by the sum of all values. \\ \hline
%7 & Canberra Distance & Calculated as the sum of absolute differences divided by the sum of the absolute values of the components. It is a weighted version of Manhattan Distance. \\ \hline
%8 & Minkowski Distance & Generalizes the Euclidean and Manhattan distances. It is controlled by a parameter p. \\ \hline
%9 & Mahalanobis Distance & Measures the distance between a point and a distribution, considering correlations between variables. \\ \hline
%10 & Wasserstein Distance & Measures the cost of transforming one probability distribution into another. \\ \hline
%\end{tabular}
%}
%\caption{Similarity and Distance Measures}
%\label{tab:similarity_measures}
%\end{table}
%
%
%
% \textbf{Combinations of Algorithms with Measures:}\\ %%%%%%%%%%%%%
%
%
%
%
\subsection{Conceptual Proposal}
\begin{enumerate}
    \item \textbf{Loading raw data from JSON files}
    \begin{itemize}
        \item Paragraphs will be extracted from academic books into a structured format, with book and paragraph index
    \end{itemize}

    \item \textbf{Including an additional dataset of book descriptions}
    \begin{itemize}
        \item A second dataset containing book metadata and textual descriptions (from Goodreads) will be used
        \item This dataset includes books from various genres such as fantasy, comedy, and romance, with the goal of testing on short-form, non-academic content
    \end{itemize}

    \item \textbf{Preprocessing and organization}
    \begin{itemize}
        \item Both datasets will be organized in DataFrames for consistent handling
        \item Text will be cleaned and normalized to prepare it for embedding
    \end{itemize}
    
    \item \textbf{Implementation of the recommendation algorithm}
    \begin{itemize}
        \item A pipeline will be developed for generating recommendations based on paragraph-to-paragraph and description-to-description similarity
        \item The system will support multiple text representation techniques
    \end{itemize}

    \item \textbf{Integration of different text representation methods}
    \begin{itemize}
        \item TF-IDF
        \item BoW (Bag of Words)
        \item LSA (Latent Semantic Analysis)
        \item GloVe
        \item FastText
        \item BERT (via SentenceTransformers)
    \end{itemize}

    \item \textbf{Running experiments and storing results}
    \begin{itemize}
        \item Each model will be executed independently on both datasets
        \item Top-N recommendations will be generated for selected input texts
    \end{itemize}

    \item \textbf{Evaluation of recommendations}
    \begin{itemize}
        \item Evaluation will be based on metrics: Similarity, Confidence, Coverage, and Diversity
        \item Time and memory usage will be tracked to compare algorithm performance
    \end{itemize}


\end{enumerate}
%
The following diagram summarizes the overall flow of the proposed system:
\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    node distance=1.2cm and 0.9cm,
    every node/.style={draw, align=center, rounded corners=5pt, minimum width=2.3cm, minimum height=1cm},
    smallnode/.style={draw, align=center, rounded corners=4pt, minimum width=1.9cm, minimum height=0.9cm},
    arrow/.style={-{Latex}, thick}
]

% Representation model nodes
\node[smallnode] (tfidf) {TF-IDF};
\node[smallnode] (bow)   [right=of tfidf] {BoW};
\node[smallnode] (lsa)   [right=of bow]   {LSA};
\node[smallnode] (glove) [right=of lsa]   {GloVe};
\node[smallnode] (fasttext) [right=of glove] {FastText};
\node[smallnode] (bert)  [right=of fasttext] {BERT};

% Preprocessing - centered between tfidf and bert
\node (preprocess) [above=1.5cm of $(tfidf)!0.5!(bert)$] {Preprocessing and Cleaning};

% Input datasets
\node (json) [above left=0.8cm and 1.2cm of preprocess] {JSON Dataset\\(paragraphs)};
\node (goodreads) [above right=0.8cm and 1.2cm of preprocess] {Goodreads Dataset\\(descriptions)};

% Recommendation pipeline
\node (pipeline) [below=2cm of $(lsa)!0.5!(glove)$] {Recommendation Pipeline};
\node (results) [below=of pipeline] {Top-N Results};
\node (evaluation) [below=of results] {Evaluation\\(Similarity, Coverage, etc.)};

% Arrows from data sources
\draw[arrow] (json.south) -- ++(0,-0.4) -| (preprocess);
\draw[arrow] (goodreads.south) -- ++(0,-0.4) -| (preprocess);

% Arrows from preprocessing to models
\draw[arrow] (preprocess) -- (tfidf);
\draw[arrow] (preprocess) -- (bow);
\draw[arrow] (preprocess) -- (lsa);
\draw[arrow] (preprocess) -- (glove);
\draw[arrow] (preprocess) -- (fasttext);
\draw[arrow] (preprocess) -- (bert);

% Arrows from models to pipeline
\draw[arrow] (tfidf) |- (pipeline);
\draw[arrow] (bow)   |- (pipeline);
\draw[arrow] (lsa)   -- (pipeline);
\draw[arrow] (glove) -- (pipeline);
\draw[arrow] (fasttext) |- (pipeline);
\draw[arrow] (bert)  |- (pipeline);

% Downstream arrows
\draw[arrow] (pipeline) -- (results);
\draw[arrow] (results) -- (evaluation);

\end{tikzpicture}
\caption{Conceptual flow of the proposed recommendation system with multiple text representation methods}
\label{fig:concept_flow}
\end{figure}






















%
%
% Tokenization, stopword removal, and optional stemming/lemmatization\\\\
%
%
%\newpage{}
%\textbf{Evaluation of Recommender Systems - Objectives and Design Space}
%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.9\textwidth]{img/evaluation_figure.png}
%    \caption{Evaluation of Recommender Systems: objectives and design space \cite{Zangerle2023}}
%    \label{fig:evaluation}
%\end{figure}
%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage{}
\section{Implementation}
% 3,4,5 Pages\\
This chapter describes how the proposed system was implemented in practice. It outlines the development environment and tools used, the data preparation process, integration of multiple text representation methods, and the overall recommendation pipeline. 

\subsection{Extracting Information and Building a Dataset}
First, the books were extracted from the digital library and stored as individual JSON files. Each file contained the entire data of a single book, further segmented into sentences, pages, and paragraphs for flexibility in experimentation.
For the full-text analysis, a dataset of 45k paragraphs was created. The average paragraph length was approximately 60-70 words, with the longest paragraph consisting of 2000 words.\\
In addition to the extracted book paragraphs, a publicly available dataset titled \textit{Books Details Dataset}, which was scraped from Goodreads \cite{goodreads_kumar_2022} was also used in the experiments. This dataset includes metadata for over 13,000 books, such as title, author, genres, ratings, and textual descriptions. The dataset contains entries with descriptions averaging approximately 160 words. These summaries were treated similarly to paragraphs during preprocessing and recommendation, allowing models to be tested on both short- and long-form content for robustness and comparison.\\



\subsection{Environment and Tools}
Jupyter Notebook\\
Virtual Machine\\
Multiprocessing\\
GPU\\
Implementation was done in Python 3.12.6\\
libraries: ...

\subsection{Used Text Represantation Models}




\subsubsection{Bert}
\begin{verbatim}
def weighted_rating(R, v, m, C):
    return (v / (v + m)) * R + (m / (v + m)) * C
\end{verbatim}



\begin{lstlisting}
def weighted_rating(R, v, m, C):
    return (v / (v + m)) * R + (m / (v + m)) * C
\end{lstlisting}



\subsection{Experiment Setup}
The \textbf{Task} is to recommend Top N books (list of books) based on 1 input book (opened book).\\\\
\textbf{Objectives:} 
\begin{itemize}
\item Implement and test multiple algorithms
\item Compare performance of algorithms using evaluation metrics
\end{itemize}

Experimenting with recommendations can be done in different ways. Here are the types of experiments to test the algorithms:
{
\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
\centering
\begin{tabular}{p{3cm}|p{10cm}}
\hline
\textbf{Type} & \textbf{Description} \\
\hline
Offline & Method: simulation of user behavior based on past interactions \\
        & Task: defined by the researcher, purely algorithmic \\
        & Repeatability: evaluation of an arbitrary number of experiments possible at low cost \\
        & Scale: large dataset, large number of users \\
        & Insights: quantitative, narrow \\
\hline
User Study & Method: user observation in live or laboratory setting \\
           & Task: defined by the researcher, carried out by the user \\
           & Repeatability: expensive \\
           & Scale: small cohort of users \\
           & Insights: quantitative and/or qualitative \\
\hline
Online & Method: real-world user observation, online field experiment \\
       & Task: self-selected by the user, carried out by the user \\
       & Repeatability: expensive \\
       & Scale: large \\
       & Insights: quantitative and/or qualitative \\
\hline
\end{tabular}

\caption{Overview of Experiment Types \cite{Zangerle2023}}
\end{table}\\
}
%
%
\textbf{Offline evaluations} are the most popular experiment type. They aim to compare different recommendation algorithms and settings and they do not require any user interaction and may be considered system-centric.  \cite{Zangerle2023}\\\\

\begin{itemize}
    \item \textbf{Technical setup and execution}
    \begin{itemize}
        \item Initial development will begin in Jupyter Notebook using Python
        \item For multiprocessing and GPU support, the system will be deployed on a Google Cloud Virtual Machine
        \item Multiprocessing will be used for CPU/GPU resource tracking
    \end{itemize}
\end{itemize}

%
%\begin{itemize}
%\item Confidence and p-values
%\item Paired Results - sign test, McNemar's test, paired Student's t-test, Wilcoxon signed rank test
%\item Unpaired Results - Mann-Whitney test
%\item Multiple Tests - ANOVA, Friedman test for ranking
%\item Confidence Intervals - Gaussian distribution, mean, standard dev
%\end{itemize}
%%
%
\section{Results and Evaluation}
This section presents how the system was evaluated using defined metrics and tracked in terms of performance.\\


Graphs\\
Tables of results\\
THRESHOLDS\\
BERT=0.5\\
BOW=0.5\\
FASTTEXT=0.95\\
GLOVE=099\\
LSA=0.5\\
TF-IDF=0.5\\
Description\\\\



\subsection{Evaluation}
The goal is to compare the previously mentioned algorithms and for comparison there are different Metrics to prove which algorithms perform better in which scenarios. When \textbf{Evaluating} a RS there are two main types of evaluation, which are:
\begin{itemize}
\item System-Centric Evaluation
	\begin{itemize}
	\item Algorithmic Aspects - e.g., the predictive accuracy of recommendation algorithms
	\end{itemize}

\item User-Centric Evaluation
	\begin{itemize}
	\item Users' Perspective - how users perceive its quality or the user experience when interacting with the RS
	\end{itemize}
\end{itemize}
%
%
I will focus on System-Centric Evaluation to test the performance of the algorithms and show results based on the metrics listed below.\\\\\\
%
%
\textbf{Metrics for evaluation:}
It is crucial to define how Relevant an item is based on the input item and it will be calculated as how similar their features are. Higher similarity will show higher relevance between the compared items in the list of recommended items. The metrics below describe how to compare the algorithms and how they use relevancy between items:
\begin{itemize}
% \item Relevance - Feature Similarity - define treshold

\item Similarity - between the vectors or embeddings

%\item Usage Prediction - capture the rate of correct recommendations - in a setting where each recommendation can be classified as relevant or non-relevant. Evaluates how well the algorithm predicts whether a user will interact with or use the recommended items, which is based on implicit feedback from the user (clicks, downloads...). Different Metrics can be used to calculate the Usage Prediction, such as Recall, Precision or F-score.

%\item Ranking Measure - evaluates the quality of the ordered list of recommendations by assessing how well the algorithm prioritizes the most relevant item. Relevant recommendations that are ranked higher are scored higher. After the ranking lists are generated by the algorithms, the Ranking Metrics are calculated from their outputs (Precision@K, NDCG@K, MRR).

\item Diversity - refers to the dissimilarity of the items recommended, where low similarity values mean high diversity. Ensures that the recommendations are not overly similar to each other and show a broader range of topics or genres. It is important to set a threshold for similarity between recommended items and the input item to make sure the recommendations are still sufficiently relevant. Firstly we need to define the attributes to assess diversity (topics, genres, keywords), then after the algorithms created the feature vectors the Similarity Measures will show how dissimilar the books in the recommendation list are from one another. Then the overall diversity score is calculated from the recommendation list, also normalized.

\item Confidence - shows the system's trust in its recommendations or predictions. It will be calculated based on Similarity between the Input item and the recommended items. After the algorithms created the feature vectors and the similarities are calculated from those, the confidence will be computed as the average similarity score across all recommended items, also normalized to a standard range [0, 1].\\
A high confidence score indicates that the recommended items are very similar to the input item, suggesting the system is confident in its recommendations.

\item Coverage - evaluates how effectively a recommendation algorithm utilizes the dataset, the proportion of items in the dataset that are recommended by the algorithm. High Item-space coverage indicates that the algorithm is exploring a wide range of items in the dataset and not favoring only a small subset. It is calculated as the percentage of: Number of recommended items / Number of total items in the dataset.

\item Ratings can show prediction

%\item Novelty - refers to the fraction of recommended items indeed new to the user.\\

\end{itemize}


DIVERSITY can be variance between the items and also can be cosidered as the contrary effect of similarity\cite{Silveira2019813}\\
Also Genre Space Coverage can be mentioned\\\\\\\\

POPULARITY - for the ratings and reviews table or graph, define that the more popular items the user gets recommended the more satisfied the user will be, higher ratings -> higher value\\\\\\

NOVELTY - \\
An alternative approach for measuring novelty is to count the number of popular
items that have been recommended [70]. This metric is based on the assumption
that highly rated and popular items are likely to be known to users and therefore
not novel [48]. A good measure for novelty might be to look more generally at how
well a recommendation system made the user aware of previously unknown items
that subsequently turn out to be useful in context [26].\cite{Avazpour2014245}\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage{}
\section{Conclusion}
% 1 - 2 pages
After implementing and testing the approaches of recommendation for books, I have found out that ...\\


%\subsection{Future Work}

\clearpage
\thispagestyle{empty}
\mbox{}


\clearpage{}
\section{Resumé}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage 
\section*{References} % No numbering
\addcontentsline{toc}{section}{References} % Add to TOC
\renewcommand{\refname}{}

\normalsize 
\bibliographystyle{unsrt} 
\bibliography{literature} 
\nocite{*}

\end{document}
