{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da85cb-f96a-4395-9fa1-ad39bc3ace16",
   "metadata": {},
   "source": [
    "# Evaluating Recommender Systems for Digital Library Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917a493-a5f9-4961-954b-899248045b84",
   "metadata": {},
   "source": [
    "## Content Based Algorithms\n",
    "## Comparison/Evaluation based on Metrics/Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2143a6a7-0379-4324-9a6b-cf1ff7eb4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81c820d9-4ec4-4dee-8193-0a1bdfa71ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-around;\">\n",
       "    <div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>No.</th>\n",
       "      <th>Feature Extraction Methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>TF-IDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Doc2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>FastText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>GloVe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>\n",
       "    <div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>No.</th>\n",
       "      <th>Similarity and Distance Measures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Euclidean Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Jaccard Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Pearson Correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Bray-Curtis Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Canberra Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Minkowski Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Mahalanobis Distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Wasserstein Distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_ext = {\"Feature Extraction Methods\":[\"TF-IDF\", \"LSA\", \"Word2Vec\", \"Doc2Vec\", \"BERT\", \"BoW\", \"BM25\", \"LDA\", \"FastText\", \"GloVe\"]}\n",
    "sim_m = {\"Similarity and Distance Measures\":[\"Cosine Similarity\", \"Euclidean Distance\", \"Jaccard Similarity\", \"Manhattan Distance\", \"Pearson Correlation\", \"Bray-Curtis Distance\", \"Canberra Distance\", \"Minkowski Distance\", \"Mahalanobis Distance\", \"Wasserstein Distance\"]}\n",
    "d_ext = pd.DataFrame(f_ext); d_ext\n",
    "d_ext.insert(0, \"No.\", range(1, len(d_ext) + 1))\n",
    "d_sim = pd.DataFrame(sim_m); d_sim\n",
    "d_sim.insert(0, \"No.\", range(1, len(d_sim) + 1))\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div>{d_ext.to_html(index=False)}</div>\n",
    "    <div>{d_sim.to_html(index=False)}</div>\n",
    "</div>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729f94b-1f97-43d6-9bce-523e5744e897",
   "metadata": {},
   "source": [
    "### Evaluation Metrics/Properties:\n",
    "- Prediction Accuracy\n",
    "    - Ratings Prediction Accuracy ? (ratings)\n",
    "    - **Usage Prediction (feedback)**\n",
    "    - **Ranking Measures**\n",
    "- **Coverage**\n",
    "- **Confidence**\n",
    "- Trust\n",
    "- **Novelty**\n",
    "- Serendipity\n",
    "- **Diversity**\n",
    "- Utility\n",
    "- Risk\n",
    "- Robustness\n",
    "- Privacy\n",
    "- Adaptability\n",
    "- Scalability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c7905-35ec-4f1f-a992-021b6b0375a3",
   "metadata": {},
   "source": [
    "### Variations of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8174b6-e452-4de7-85f8-ce1f4520e633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8488b9-8dff-4318-85e1-85e8a9e6694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f762c8f-19e2-4c7a-89f7-287d14e736d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc5f3a1-44ec-4dbe-9831-074e2d354bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76c0fdce-aef4-487f-bcaa-1df7ba9adf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0425176428</td>\n",
       "      <td>What If?: The World's Foremost Military Histor...</td>\n",
       "      <td>Robert Cowley</td>\n",
       "      <td>2000</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0671870432</td>\n",
       "      <td>PLEADING GUILTY</td>\n",
       "      <td>Scott Turow</td>\n",
       "      <td>1993</td>\n",
       "      <td>Audioworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0679425608</td>\n",
       "      <td>Under the Black Flag: The Romance and the Real...</td>\n",
       "      <td>David Cordingly</td>\n",
       "      <td>1996</td>\n",
       "      <td>Random House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>074322678X</td>\n",
       "      <td>Where You'll Find Me: And Other Stories</td>\n",
       "      <td>Ann Beattie</td>\n",
       "      <td>2002</td>\n",
       "      <td>Scribner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                              Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "5  0399135782                             The Kitchen God's Wife   \n",
       "6  0425176428  What If?: The World's Foremost Military Histor...   \n",
       "7  0671870432                                    PLEADING GUILTY   \n",
       "8  0679425608  Under the Black Flag: The Romance and the Real...   \n",
       "9  074322678X            Where You'll Find Me: And Other Stories   \n",
       "\n",
       "                 Author  Year                 Publisher  \n",
       "0    Mark P. O. Morford  2002   Oxford University Press  \n",
       "1  Richard Bruce Wright  2001     HarperFlamingo Canada  \n",
       "2          Carlo D'Este  1991           HarperPerennial  \n",
       "3      Gina Bari Kolata  1999      Farrar Straus Giroux  \n",
       "4       E. J. W. Barber  1999    W. W. Norton & Company  \n",
       "5               Amy Tan  1991          Putnam Pub Group  \n",
       "6         Robert Cowley  2000  Berkley Publishing Group  \n",
       "7           Scott Turow  1993                Audioworks  \n",
       "8       David Cordingly  1996              Random House  \n",
       "9           Ann Beattie  2002                  Scribner  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Books/books.csv', delimiter=';')\n",
    "\n",
    "df_head = df.head(10); df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c879ea-b10d-41ba-ae89-f4115507ac53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5710a12-dc62-45a2-b8e5-49a61e7b6f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'The Matrix':\n",
      "2    The Matrix Revolutions\n",
      "1       The Matrix Reloaded\n",
      "4              Interstellar\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'title': [\n",
    "        'The Matrix', \n",
    "        'The Matrix Reloaded', \n",
    "        'The Matrix Revolutions', \n",
    "        'Inception', \n",
    "        'Interstellar', \n",
    "        'The Prestige'\n",
    "    ],\n",
    "    'description': [\n",
    "        'A computer hacker learns about the true nature of reality and his role in the war against its controllers.',\n",
    "        'Neo and his allies race against time before the machines come to destroy Zion.',\n",
    "        'The human city of Zion defends itself against the massive invasion of the machines.',\n",
    "        'A thief who steals corporate secrets through dream-sharing technology is given an inverse task.',\n",
    "        'A team of explorers travels through a wormhole in space in an attempt to save humanity.',\n",
    "        'Two stage magicians engage in a battle to create the ultimate illusion.'\n",
    "    ],\n",
    "    'genres': [\n",
    "        'Action, Sci-Fi', \n",
    "        'Action, Sci-Fi', \n",
    "        'Action, Sci-Fi', \n",
    "        'Sci-Fi, Thriller', \n",
    "        'Sci-Fi, Drama', \n",
    "        'Drama, Mystery'\n",
    "    ],\n",
    "    'actors': [\n",
    "        'Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss',\n",
    "        'Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss',\n",
    "        'Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss',\n",
    "        'Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page',\n",
    "        'Matthew McConaughey, Anne Hathaway, Jessica Chastain',\n",
    "        'Hugh Jackman, Christian Bale, Scarlett Johansson'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Combine Features\n",
    "def combine_features(row):\n",
    "    return f\"{row['description']} {row['genres']} {row['actors']}\"\n",
    "\n",
    "df['combined_features'] = df.apply(combine_features, axis=1)\n",
    "\n",
    "# Compute TF-IDF matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['combined_features'])\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = df.index[df['title'] == title].tolist()[0]\n",
    "    \n",
    "    # Get the pairwise similarity scores for all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # Sort the movies based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores of the 3 most similar movies (excluding the first one, which is itself)\n",
    "    sim_scores = sim_scores[1:4]\n",
    "    \n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top 3 most similar movies\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "\n",
    "movie_title = \"The Matrix\"\n",
    "recommendations = get_recommendations(movie_title)\n",
    "print(f\"Recommendations for '{movie_title}':\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80630da7-c120-424b-bd3d-67b68f73b96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7565a4-7f84-4ece-97f5-23dd5ded932a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "335ba40d-8be1-4c0d-acab-5c579b0ea9a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LSA and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6216e17e-b97a-4849-b4f1-950d55f4869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books similar to 'Book A':\n",
      "- Book D (similarity score: 1.00)\n",
      "- Book C (similarity score: 0.00)\n",
      "- Book E (similarity score: -0.00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample book metadata\n",
    "data = {\n",
    "    'BookID': [1, 2, 3, 4, 5],\n",
    "    'Title': ['Book A', 'Book B', 'Book C', 'Book D', 'Book E'],\n",
    "    'Description': [\n",
    "        'A tale of adventure and mystery in a fantastical world.',\n",
    "        'A romantic drama set in the heart of the city.',\n",
    "        'A science fiction novel exploring space and time.',\n",
    "        'A thrilling mystery with unexpected twists.',\n",
    "        'A heartwarming story about love and friendship.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "books_df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess the text\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(books_df['Description'])\n",
    "\n",
    "# Apply LSA (Latent Semantic Analysis)\n",
    "lsa = TruncatedSVD(n_components=2, random_state=42)\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Compute cosine similarity between books\n",
    "similarity_matrix = cosine_similarity(lsa_matrix)\n",
    "\n",
    "# recommend books based on a given book title\n",
    "def recommend_books(title, top_n=3):\n",
    "    if title not in books_df['Title'].values:\n",
    "        return f\"Book '{title}' not found in the database.\"\n",
    "    \n",
    "    book_index = books_df.index[books_df['Title'] == title].tolist()[0]\n",
    "    similar_books = list(enumerate(similarity_matrix[book_index]))\n",
    "    similar_books = sorted(similar_books, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    recommendations = []\n",
    "    for i, (idx, sim_score) in enumerate(similar_books[1:top_n+1]):\n",
    "        recommendations.append((books_df.iloc[idx]['Title'], sim_score))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test\n",
    "book_title = 'Book A'\n",
    "recommended_books = recommend_books(book_title, top_n=3)\n",
    "print(f\"Books similar to '{book_title}':\")\n",
    "for title, score in recommended_books:\n",
    "    print(f\"- {title} (similarity score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e654681-2078-46b1-9a06-7b586d0bd509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da09d7-d4de-4333-b1ff-384089530af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63c5432a-7de1-4b6c-8567-167a532331fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1548b6e0-589a-456b-9654-102a21880bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def train_word2vec(data):\n",
    "    sentences = [desc.split() for desc in data['description']]\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def recommend_word2vec(book_title, data, model):\n",
    "    def vectorize(text):\n",
    "        words = text.split()\n",
    "        vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "    \n",
    "    book_vector = vectorize(data[data['title'] == book_title]['description'].iloc[0])\n",
    "    similarities = []\n",
    "    for i, desc in enumerate(data['description']):\n",
    "        similarities.append((i, cosine_similarity([book_vector], [vectorize(desc)])[0][0]))\n",
    "    \n",
    "    sorted_scores = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [data['title'][i[0]] for i in sorted_scores[1:6]]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29063357-7ecb-48e6-a796-7e5d0849691a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddcd77d-23dc-4e1d-b1ea-00b79e4f3f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ee6c61-ce96-4bef-9e79-f5b9c5ac7528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591633cc-6390-42b8-9161-f6a4b6f40dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def train_doc2vec(data):\n",
    "    documents = [TaggedDocument(desc.split(), [i]) for i, desc in enumerate(data['description'])]\n",
    "    model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def recommend_doc2vec(book_title, data, model):\n",
    "    book_vector = model.infer_vector(data[data['title'] == book_title]['description'].iloc[0].split())\n",
    "    similarities = []\n",
    "    for i, desc in enumerate(data['description']):\n",
    "        desc_vector = model.infer_vector(desc.split())\n",
    "        similarities.append((i, cosine_similarity([book_vector], [desc_vector])[0][0]))\n",
    "    \n",
    "    sorted_scores = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [data['title'][i[0]] for i in sorted_scores[1:6]]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0cd34-592f-413d-b37b-1f29a6d6a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9043423-5ebe-48e3-949d-e9379f0e1209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b75c6fc-cd90-42e6-8d99-fd28d496e339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97f97e53-ec88-4b22-82d1-03bfdf0f5825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/203.0 MB 9.1 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.7/203.0 MB 8.4 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 5.5/203.0 MB 8.8 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 7.6/203.0 MB 9.0 MB/s eta 0:00:22\n",
      "   - -------------------------------------- 9.4/203.0 MB 9.2 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 10.5/203.0 MB 9.2 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 12.3/203.0 MB 8.6 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 14.7/203.0 MB 8.9 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 16.8/203.0 MB 9.0 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 18.9/203.0 MB 9.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 21.0/203.0 MB 9.3 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 23.3/203.0 MB 9.4 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 25.4/203.0 MB 9.5 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 27.8/203.0 MB 9.6 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 29.9/203.0 MB 9.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 32.2/203.0 MB 9.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 34.3/203.0 MB 9.8 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 36.7/203.0 MB 9.9 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 39.1/203.0 MB 10.0 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 41.4/203.0 MB 10.0 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 43.3/203.0 MB 10.0 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 45.4/203.0 MB 10.0 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 47.7/203.0 MB 10.0 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 50.1/203.0 MB 10.1 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 52.2/203.0 MB 10.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 53.7/203.0 MB 10.1 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 56.1/203.0 MB 10.1 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 57.9/203.0 MB 10.1 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 60.0/203.0 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 62.1/203.0 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 64.2/203.0 MB 10.0 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 66.3/203.0 MB 10.0 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 68.7/203.0 MB 10.1 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 71.0/203.0 MB 10.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 73.1/203.0 MB 10.1 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 75.5/203.0 MB 10.1 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 77.6/203.0 MB 10.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 80.0/203.0 MB 10.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 82.1/203.0 MB 10.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 84.1/203.0 MB 10.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 86.0/203.0 MB 10.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 88.1/203.0 MB 10.1 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 90.2/203.0 MB 10.1 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 92.3/203.0 MB 10.2 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 94.4/203.0 MB 10.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 96.5/203.0 MB 10.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 98.6/203.0 MB 10.2 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 100.9/203.0 MB 10.2 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 103.3/203.0 MB 10.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 105.1/203.0 MB 10.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 107.2/203.0 MB 10.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 109.3/203.0 MB 10.2 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 111.4/203.0 MB 10.2 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 113.5/203.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 115.9/203.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 117.7/203.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 119.8/203.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 122.2/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 124.3/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 126.9/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 129.0/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 130.8/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 133.2/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 135.5/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 137.9/203.0 MB 10.3 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 140.2/203.0 MB 10.3 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 142.6/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 145.0/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 147.6/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 149.9/203.0 MB 10.4 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 152.3/203.0 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 154.7/203.0 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 157.3/203.0 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 159.6/203.0 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 162.0/203.0 MB 10.5 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 164.4/203.0 MB 10.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 167.0/203.0 MB 10.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 169.3/203.0 MB 10.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 171.7/203.0 MB 10.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.1/203.0 MB 10.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 175.9/203.0 MB 10.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 178.0/203.0 MB 10.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 180.1/203.0 MB 10.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 182.7/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 184.8/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 187.2/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 189.3/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.1/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 193.2/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 195.3/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.4/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.5/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.3/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.0/203.0 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.4/6.2 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/10.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.4/2.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 10.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, safetensors, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 sentence-transformers-3.3.1 sympy-1.13.1 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c69effe0-8e39-4f18-be3b-5f951b07e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def recommend_bert(book_title, data):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(data['description'].tolist())\n",
    "    \n",
    "    book_embedding = model.encode(data[data['title'] == book_title]['description'].iloc[0])\n",
    "    similarities = cosine_similarity([book_embedding], embeddings)[0]\n",
    "    \n",
    "    sorted_scores = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [data['title'][i[0]] for i in sorted_scores[1:6]]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc92d1-5aac-49fe-b1d2-2bb4b02e8f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6824d-d9cc-4208-bfbc-895d786c3d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a47790-a9e5-4aa4-b8c8-3441bd400828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c6272bd-7480-420a-a22e-86d5e393a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def recommend_bow(book_title, data):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    bow_matrix = count_vectorizer.fit_transform(data['description'])\n",
    "    \n",
    "    cosine_sim = cosine_similarity(bow_matrix, bow_matrix)\n",
    "    idx = data[data['title'] == book_title].index[0]\n",
    "    scores = list(enumerate(cosine_sim[idx]))\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    recommendations = [data['title'][i[0]] for i in sorted_scores[1:6]]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf47de7-a358-44cd-87c8-51ec8e48d42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba1c47-fe25-4fcb-b597-12fedfaa9ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53dbfc0-3dd0-4371-bf5f-618164641a92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02f8546c-f2f4-4fee-b8da-551cd91ff633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\akos levardy\\anaconda3\\lib\\site-packages (from rank-bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ef64226-40a1-43ed-8ba3-32e522d515ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def recommend_bm25(book_title, data):\n",
    "    tokenized_corpus = [desc.split() for desc in data['description']]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    query = data[data['title'] == book_title]['description'].iloc[0].split()\n",
    "    scores = bm25.get_scores(query)\n",
    "    \n",
    "    sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [data['title'][i[0]] for i in sorted_scores[1:6]]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24345dc6-4e27-401b-930e-f5c501230be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640eeb87-7d3d-43e3-b64b-ecc8e057b7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c23134-7e9c-4750-8fa7-17015e480009",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "384566f9-d1be-411e-9c34-7e7859fbbf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Recommendations: ['Book2', 'Book5', 'Book4', 'Book3']\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def lda_recommend(data, book_title, num_topics=10):\n",
    "    # Preprocessing: Tokenize descriptions\n",
    "    tokenized_descriptions = [desc.split() for desc in data['description']]\n",
    "    \n",
    "    # Create a dictionary and corpus for LDA\n",
    "    dictionary = Dictionary(tokenized_descriptions)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_descriptions]\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Get topic distributions for each document\n",
    "    topic_distributions = [lda.get_document_topics(bow, minimum_probability=0) for bow in corpus]\n",
    "    topic_vectors = np.array([[prob for _, prob in dist] for dist in topic_distributions])\n",
    "    \n",
    "    # Get the topic vector for the queried book\n",
    "    book_idx = data[data['title'] == book_title].index[0]\n",
    "    book_vector = topic_vectors[book_idx]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity([book_vector], topic_vectors)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Return top 5 recommendations\n",
    "    recommendations = data['title'].iloc[sorted_indices[1:6]].tolist()\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'title': ['Book1', 'Book2', 'Book3', 'Book4', 'Book5'],\n",
    "    'description': [\n",
    "        'A story about friendship and adventure.',\n",
    "        'A thrilling mystery novel with unexpected twists.',\n",
    "        'A guide to understanding the basics of quantum physics.',\n",
    "        'A romantic tale set in the beautiful Italian countryside.',\n",
    "        'An action-packed fantasy with dragons and knights.'\n",
    "    ]\n",
    "})\n",
    "\n",
    "book_title = 'Book1'\n",
    "print(\"LDA Recommendations:\", lda_recommend(data, book_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719e76e-9c30-44fa-b2bc-092adb6e70c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59270916-2699-4203-a07f-6b25e1a09e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9892ee7-a6c4-4cd1-a2dd-338048c4a67d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8fd9b4e-c1c4-4418-9287-0e69a34e9b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText Recommendations: ['Book2', 'Book4', 'Book3', 'Book5']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def fasttext_recommend(data, book_title):\n",
    "    # Preprocessing: Tokenize descriptions\n",
    "    tokenized_descriptions = [desc.split() for desc in data['description']]\n",
    "    \n",
    "    # Train the FastText model\n",
    "    model = FastText(tokenized_descriptions, vector_size=100, window=5, min_count=1, epochs=10)\n",
    "    \n",
    "    # Get document vectors by averaging word embeddings\n",
    "    def get_document_vector(tokens):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "    \n",
    "    doc_vectors = np.array([get_document_vector(tokens) for tokens in tokenized_descriptions])\n",
    "    \n",
    "    # Get the vector for the queried book\n",
    "    book_idx = data[data['title'] == book_title].index[0]\n",
    "    book_vector = doc_vectors[book_idx]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity([book_vector], doc_vectors)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Return top 5 recommendations\n",
    "    recommendations = data['title'].iloc[sorted_indices[1:6]].tolist()\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "print(\"FastText Recommendations:\", fasttext_recommend(data, book_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078a11f-806f-4346-b205-2af7ce0fd52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e437d6-dd91-442d-8303-6d1dcd9c98b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9e19fc-49fc-4f7b-b1f9-972258eca6d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774339a3-ccbb-46be-bf65-94284f80a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vectors\n",
    "    return embeddings_index\n",
    "\n",
    "def glove_recommend(data, book_title, glove_path='glove.6B.100d.txt'):\n",
    "    # Load pre-trained GloVe embeddings\n",
    "    embeddings_index = load_glove_embeddings(glove_path)\n",
    "    \n",
    "    # Preprocessing: Tokenize descriptions\n",
    "    tokenized_descriptions = [desc.split() for desc in data['description']]\n",
    "    \n",
    "    # Get document vectors by averaging GloVe word embeddings\n",
    "    def get_document_vector(tokens):\n",
    "        vectors = [embeddings_index[word] for word in tokens if word in embeddings_index]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # 100 for GloVe 100d\n",
    "    \n",
    "    doc_vectors = np.array([get_document_vector(tokens) for tokens in tokenized_descriptions])\n",
    "    \n",
    "    # Get the vector for the queried book\n",
    "    book_idx = data[data['title'] == book_title].index[0]\n",
    "    book_vector = doc_vectors[book_idx]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity([book_vector], doc_vectors)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Return top 5 recommendations\n",
    "    recommendations = data['title'].iloc[sorted_indices[1:6]].tolist()\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "glove_path = 'path_to_glove/glove.6B.100d.txt'  # Update with the actual path\n",
    "print(\"GloVe Recommendations:\", glove_recommend(data, book_title, glove_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1579bfc-3567-4006-a3db-f9bec5131322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5a3fa-7a8b-4553-b02f-e1e1b3f29bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b6d7d-d76d-47c0-9398-04c7fa03501d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179d400-4b9d-41d1-886f-f29d41ef29c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eacfe4-9e82-4af4-ac69-c3273e27ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53809958-44f3-44ad-86f9-68b45b62fff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf4f5-4985-42fb-bb55-61a88af147ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5037cc-8198-4a28-9ade-d5994de58db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
